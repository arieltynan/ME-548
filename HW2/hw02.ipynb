{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "### Instructions\n",
    "Download this jupyer notebook (button at the top of the page or download from the Github repository). Provide your answers as Markdown text, Python code, and/or produce plots as appropriate. The notebook should run all the cells in order without errors.  \n",
    "Submit both the `.ipynb` and a `.pdf` to Canvas.\n",
    "\n",
    "Make sure the `.pdf` has all the relevant outputs showing. To save as `.pdf` you can first export the notebook as `.html`, open it in a browers and then \"Print to PDF\". \n",
    "\n",
    "**NOTE:** As we will be sharing the files for peer grading, please keep your submission anonymous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (Stochastic dynamic programming)\n",
    "*(Adapted from Stanford AA 203)*\n",
    "\n",
    "In this problem we will explore discrete-time dynamic programming for stochastic systems; that is, systems where the result of taking a certain action is not deterministic, but instead any of a set of results may occur, according to some known probability distribution. In this case, we cannot optimize the value function directly, since even choosing a known sequence of actions will not always give in the same result. Instead, we optimize the [_expected value_](https://en.wikipedia.org/wiki/Expected_value) of the value function instead (if it's been a while since you've taken a probability class, or if you've never taken one, that Wikipedia article may be helpful).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Small hand-calculation problem\n",
    "\n",
    "Suppose we have a machine that is either running or is broken down. If it runs throughout one\n",
    "week, it makes a gross profit of \\$100. If it fails during the week, gross profit is zero. If it is running at the start of the week and we perform preventive maintenance, the probability that it will fail during the week is 0.4. If we do not perform such maintenance, the probability of failure is 0.7. However, maintenance will cost \\$20. If the machine is broken down at the start of the week, it may either be repaired at a cost of \\$40, in which case it will fail during the week with a probability\n",
    "of 0.4, or it may be replaced at a cost of \\$150 by a new machine; a new machine is guaranteed to run through its first week of operation. Using dynamic programming, find the optimal repair, replacement, and maintenance policy that maximizes total expected profit over four weeks, assuming a new machine at the start of the first week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including image of hand calculations in PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Larger system to solve by code\n",
    "\n",
    "Now we consider a more complicated system and a longer time horizon.\n",
    "\n",
    "Consider the same scenario as above, but with two additional machine states: overspeeding and destroyed. In the overspeeding state, the machine will with probability 0.5 produce \\$120 at the end of the week, but will otherwise be destroyed and produce no revenue for that week. If the machine is in the \"destroyed\" state at the start of a week, it may be replaced with a new machine for \\$150, the same as if it were broken down; otherwise it will produce no revenue for that week and remain in the destroyed state. A destroyed machine may not be repaired.\n",
    "\n",
    "Here are the state transitions possible in this new system:\n",
    "\n",
    "- If the machine is in the \"running\" state at the start of the week:\n",
    "    - If you do nothing (cost: \\$0)\n",
    "        - With probability 0.3 it will produce \\$100 and remain in the \"running\" state at the end of the week.\n",
    "        - With probabiity 0.63 it will produce \\$0 and enter the \"broken down\" state at the end of the week.\n",
    "        - With probability 0.07 it will produce \\$100 and enter the \"overspeeding\" state at the end of the week.\n",
    "    - If you maintain the machine (cost: \\$20):\n",
    "        - With probability 0.6 it will produce \\$100 and remain in the \"running\" state at the end of the week.\n",
    "        - With probabiity 0.37 it will produce \\$0 and enter the \"broken down\" state at the end of the week.\n",
    "        - With probability 0.03 it will produce \\$100 and enter the \"overspeeding\" state at the end of the week.\n",
    "- If the machine is in the \"broken down\" state at the start of the week:\n",
    "    - If you do nothing (cost: \\$0):\n",
    "        - The machine will produce \\$0, and will remain in the \"broken down\" state at the end of the week.\n",
    "    - If you repair the machine (cost: \\$40):\n",
    "        - With probability 0.6 it will produce \\$100 and remain in the \"running\" state at the end of the week.\n",
    "        - With probabiity 0.37 it will produce \\$0 and enter the \"broken down\" state at the end of the week.\n",
    "        - With probability 0.03 it will produce \\$100 and enter the \"overspeeding\" state at the end of the week.\n",
    "    - If you replace the machine (cost: \\$150):\n",
    "        - The new machine will produce \\$100 and be in the \"running\" state at the end of the week.\n",
    "- If the machine is in the \"overspeeding\" state at the start of the week:\n",
    "    - If you do nothing (cost: \\$0):\n",
    "        - With probability 0.5 it will produce \\$120 and remain in the \"overspeeding\" state at the end of the week.\n",
    "        - With probability 0.5 it will produce \\$0 and enter the \"destroyed\" state at the end of the week.\n",
    "    - If you repair the machine (cost: \\$40):\n",
    "        - With probability 0.6 it will produce \\$100 and remain in the \"running\" state at the end of the week.\n",
    "        - With probabiity 0.37 it will produce \\$0 and enter the \"broken down\" state at the end of the week.\n",
    "        - With probability 0.03 it will produce \\$100 and enter the \"overspeeding\" state at the end of the week.\n",
    "- If the machine is in the \"destroyed\" state at the start of the week:\n",
    "    - If you do nothing (cost: \\$0):\n",
    "        - The machine will produce \\$0 and remain in the \"destroyed\" state at the end of the week.\n",
    "    - If you replace the machine (cost: \\$150):\n",
    "        - The new machine will produce \\$100 and be in the \"running\" state at the end of the week.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Suppose that by the end of the 20th week (i.e., start of the 21st week), the machine is still \"running\", then you can sell the machine for \\$200. \n",
    "If the machine is \"overspeeding\", the machine will sell for \\$120.\n",
    "If the machine is \"broken down\", the machine will sell for \\$30.\n",
    "If the machine is \"destroyed\", then you must pay for a recycling fee of \\$50.\n",
    "\n",
    "In the following parts, you will implement the dynamic programming algorithm to find the optimal action to take in each state in each week, as well as the optimal expected profit in each state in each week. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)(i) Quick hand calculation\n",
    "Let's start by considering just the last week and computing the first dynamic programming step by hand.\n",
    "What is the value at the start of week 21? That is, what is the terminal value?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The terminal values are given for week 21. Running = 200, Overspeeding = 120, Broken Down = 30, Destroyed = 50.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)(ii)\n",
    "Given that, what is the value at the start of week 20 and the corresponding optimal policy?    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a picture to the PDF file. The hand-calculated optimal policy is:\n",
    "\n",
    "Running + Maintenance: 134.7\n",
    "Breakdown + Replace: 200\n",
    "Overspeeding + Repair: 134.7\n",
    "Destroyed + Replace: 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)(iii)\n",
    "\n",
    "Now, fill in the following functions to compute the value function and optimal policy over the 20 weeks.\n",
    "\n",
    "Print out the optimal policy and value function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the states and actions of the system, and corresponding index \n",
    "STATES = {\"RUNNING\": 0, \"BROKEN_DOWN\": 1, \"OVERSPEEDING\": 2, \"DESTROYED\": 3}\n",
    "ACTIONS = {\"NOTHING\": 0, \"MAINTAIN\": 1, \"REPAIR\": 2, \"REPLACE\": 3}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_transition_probability_matrix(STATES, ACTIONS):\n",
    "    \"\"\"\n",
    "    Construct the transition probability matrix for the car maintenance problem.\n",
    "    The transition probability matrix is a 3D array where the first dimension\n",
    "    represents the current state, the second dimension represents the next state,\n",
    "    and the third dimension represents the action taken.\n",
    "    \"\"\"\n",
    "\n",
    "    tpm = np.zeros((len(STATES),len(STATES),len(ACTIONS))) #4x4x4 3D array #transitional probability matrix\n",
    "\n",
    "    #Runnning, Nothing\n",
    "    tpm[STATES[\"RUNNING\"]][STATES[\"RUNNING\"]][ACTIONS[\"NOTHING\"]] = 0.3 # running\n",
    "    tpm[STATES[\"RUNNING\"]][STATES[\"BROKEN_DOWN\"]][ACTIONS[\"NOTHING\"]] = 0.63 # bd\n",
    "    tpm[STATES[\"RUNNING\"]][STATES[\"OVERSPEEDING\"]][ACTIONS[\"NOTHING\"]] = 0.07 # os\n",
    "\n",
    "    #Running, maintain\n",
    "    tpm[STATES[\"RUNNING\"]][STATES[\"RUNNING\"]][ACTIONS[\"MAINTAIN\"]] = 0.6 # running running maintain\n",
    "    tpm[STATES[\"RUNNING\"]][STATES[\"BROKEN_DOWN\"]][ACTIONS[\"MAINTAIN\"]] = 0.37 # bd\n",
    "    tpm[STATES[\"RUNNING\"]][STATES[\"OVERSPEEDING\"]][ACTIONS[\"MAINTAIN\"]] = 0.03 # os\n",
    "\n",
    "    #Broken down, nothing\n",
    "    tpm[STATES[\"BROKEN_DOWN\"]][STATES[\"BROKEN_DOWN\"]][ACTIONS[\"NOTHING\"]] = 1 #broken down\n",
    "\n",
    "    #broken down, repair\n",
    "    tpm[STATES[\"BROKEN_DOWN\"]][STATES[\"RUNNING\"]][ACTIONS[\"REPAIR\"]] = 0.6\n",
    "    tpm[STATES[\"BROKEN_DOWN\"]][STATES[\"BROKEN_DOWN\"]][ACTIONS[\"REPAIR\"]] = 0.37\n",
    "    tpm[STATES[\"BROKEN_DOWN\"]][STATES[\"OVERSPEEDING\"]][ACTIONS[\"REPAIR\"]] = 0.03\n",
    "\n",
    "    #Overspeeding, repair\n",
    "    tpm[STATES[\"OVERSPEEDING\"]][STATES[\"RUNNING\"]][ACTIONS[\"REPAIR\"]] = 0.6\n",
    "    tpm[STATES[\"OVERSPEEDING\"]][STATES[\"BROKEN_DOWN\"]][ACTIONS[\"REPAIR\"]] = 0.37\n",
    "    tpm[STATES[\"OVERSPEEDING\"]][STATES[\"OVERSPEEDING\"]][ACTIONS[\"REPAIR\"]] = 0.03\n",
    "\n",
    "    #Replaced\n",
    "    tpm[STATES[\"BROKEN_DOWN\"]][STATES[\"RUNNING\"]][ACTIONS[\"REPLACE\"]] = 1 #BD\n",
    "\n",
    "    tpm[STATES[\"DESTROYED\"]][STATES[\"RUNNING\"]][[ACTIONS[\"REPLACE\"]]] = 1 #DS\n",
    "\n",
    "    #Overspeeding, nothing\n",
    "    tpm[STATES[\"OVERSPEEDING\"]][STATES[\"OVERSPEEDING\"]][ACTIONS[\"NOTHING\"]] = 0.5\n",
    "    tpm[STATES[\"OVERSPEEDING\"]][STATES[\"DESTROYED\"]][ACTIONS[\"NOTHING\"]] = 0.5\n",
    "\n",
    "    #Destroyed, nothing\n",
    "    tpm[STATES[\"DESTROYED\"]][STATES[\"DESTROYED\"]][ACTIONS[\"NOTHING\"]] = 1 #DS \n",
    " \n",
    "    return tpm\n",
    "    ########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_reward_matrix(STATES, ACTIONS):\n",
    "    \"\"\"\n",
    "    Construct the reward matrix for the car maintenance problem.\n",
    "    The reward matrix is a 3D array where the first dimension\n",
    "    represents the current state, the second dimension represents the next state,\n",
    "    and the third dimension represents the action taken.\n",
    "    \"\"\" \n",
    "    rm = np.zeros((len(STATES),len(STATES),len(ACTIONS))) #4x4x4 3D array #reward matrix\n",
    "    \n",
    "    #Runnning, Nothing\n",
    "    rm[STATES[\"RUNNING\"]][STATES[\"RUNNING\"]][ACTIONS[\"NOTHING\"]] = 100 # running\n",
    "    rm[STATES[\"RUNNING\"]][STATES[\"BROKEN_DOWN\"]][ACTIONS[\"NOTHING\"]] = 0 # bd\n",
    "    rm[STATES[\"RUNNING\"]][STATES[\"OVERSPEEDING\"]][ACTIONS[\"NOTHING\"]] = 100 # os\n",
    "\n",
    "    #Running, maintain\n",
    "    rm[STATES[\"RUNNING\"]][STATES[\"RUNNING\"]][ACTIONS[\"MAINTAIN\"]] = 100 - 20 # running running maintain\n",
    "    rm[STATES[\"RUNNING\"]][STATES[\"BROKEN_DOWN\"]][ACTIONS[\"MAINTAIN\"]] = 0 - 20 # bd\n",
    "    rm[STATES[\"RUNNING\"]][STATES[\"OVERSPEEDING\"]][ACTIONS[\"MAINTAIN\"]] = 100 - 20 # os\n",
    "\n",
    "    #Broken down, nothing\n",
    "    rm[STATES[\"BROKEN_DOWN\"]][STATES[\"BROKEN_DOWN\"]][ACTIONS[\"NOTHING\"]] = 0 #broken down\n",
    "\n",
    "    #broken down, repair\n",
    "    rm[STATES[\"BROKEN_DOWN\"]][STATES[\"RUNNING\"]][ACTIONS[\"REPAIR\"]] = 100 - 40\n",
    "    rm[STATES[\"BROKEN_DOWN\"]][STATES[\"BROKEN_DOWN\"]][ACTIONS[\"REPAIR\"]] = 0 - 40\n",
    "    rm[STATES[\"BROKEN_DOWN\"]][STATES[\"OVERSPEEDING\"]][ACTIONS[\"REPAIR\"]] = 100 - 40\n",
    "\n",
    "    #Overspeeding, repair\n",
    "    rm[STATES[\"OVERSPEEDING\"]][STATES[\"RUNNING\"]][ACTIONS[\"REPAIR\"]] = 100 - 40\n",
    "    rm[STATES[\"OVERSPEEDING\"]][STATES[\"BROKEN_DOWN\"]][ACTIONS[\"REPAIR\"]] = 0 - 40\n",
    "    rm[STATES[\"OVERSPEEDING\"]][STATES[\"OVERSPEEDING\"]][ACTIONS[\"REPAIR\"]] = 100 - 40\n",
    "\n",
    "    #Replaced\n",
    "    rm[STATES[\"BROKEN_DOWN\"]][STATES[\"RUNNING\"]][ACTIONS[\"REPLACE\"]] = 100 - 150 #BD\n",
    "\n",
    "    rm[STATES[\"DESTROYED\"]][STATES[\"RUNNING\"]][ACTIONS[\"REPLACE\"]] = 100 - 150 #DS\n",
    "\n",
    "    #Overspeeding, nothing\n",
    "    rm[STATES[\"OVERSPEEDING\"]][STATES[\"OVERSPEEDING\"]][ACTIONS[\"NOTHING\"]] = 120 # overspeeding\n",
    "    rm[STATES[\"OVERSPEEDING\"]][STATES[\"DESTROYED\"]][ACTIONS[\"NOTHING\"]] = 0  #broken down\n",
    "\n",
    "    #Destroyed, nothing\n",
    "    rm[STATES[\"DESTROYED\"]][STATES[\"DESTROYED\"]][ACTIONS[\"NOTHING\"]] = 0 #DS\n",
    "\n",
    "    return rm\n",
    "    ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allowable_action_set(state):\n",
    "    \"\"\"\n",
    "    Returns the set of actions that are allowed in the given state.\n",
    "    \"\"\"\n",
    "    if state == \"RUNNING\":\n",
    "        return [\"NOTHING\", \"MAINTAIN\"]\n",
    "    elif state == \"BROKEN_DOWN\":\n",
    "        return [\"NOTHING\", \"REPAIR\", \"REPLACE\"]\n",
    "    elif state == \"OVERSPEEDING\":\n",
    "        return [\"NOTHING\", \"REPAIR\"]\n",
    "    elif state == \"DESTROYED\":\n",
    "        return [\"NOTHING\", \"REPLACE\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_matrix = construct_transition_probability_matrix(STATES, ACTIONS)\n",
    "reward_matrix = construct_reward_matrix(STATES, ACTIONS)\n",
    "\n",
    "n_weeks = 20\n",
    "\n",
    "V = np.zeros((len(STATES), n_weeks+1))\n",
    "V[:,-1] = np.array([200, 30, 120, -50])  # RUNNING, BROKEN_DOWN, OVERSPEEDING, DESTROYED\n",
    "\n",
    "policy = {}\n",
    "for t in range(n_weeks + 1):  # Initialize policy for all time steps\n",
    "    for curr_state_name, curr_state_num in STATES.items():\n",
    "        policy[(curr_state_name, t)] = None  # Set a default value (None) for all states\n",
    "\n",
    "##### FILL ME IN #####\n",
    "# update value function for each state and time step\n",
    "# Use the Bellman equation to update the value function\n",
    "\n",
    "for t in reversed(range(n_weeks)): # iterating backwards\n",
    "    for curr_state_name, curr_state_num in STATES.items(): #for every state in dict STATES\n",
    "        max_value = float('-inf') # initial max value starts at the minimum so it can be updated\n",
    "        best_action = None # init best actions\n",
    "\n",
    "        #iterate through each action at state\n",
    "        for action_name, action_num in ACTIONS.items(): # for every action in dict ACTIONS\n",
    "            expected_value = 0 #init expected valye\n",
    "            for next_state_name, next_state_num in STATES.items(): # for every state in dict STATES (next states)\n",
    "                \n",
    "                prob = probability_matrix[curr_state_num][next_state_num][action_num]\n",
    "                reward = reward_matrix[curr_state_num][next_state_num][action_num]\n",
    "                expected_value += prob * (reward + V[next_state_num][t + 1]) # sum prob dist of costs of action movement to next state for a specifc option\n",
    "            \n",
    "            #total_value = reward + expected_value # Add reward /term cost/ at the next state\n",
    "\n",
    "            if expected_value > max_value:\n",
    "                max_value = expected_value\n",
    "                best_action = action_name\n",
    "\n",
    "        V[curr_state_num][t] = max_value #max value at timestep t\n",
    "        policy[(curr_state_name, t)] = best_action #best action at tiemstep t\n",
    "\n",
    "\n",
    "##################\n",
    "\n",
    "\n",
    "##### UNCOMMENT THE FOLLOWING CODE #####\n",
    "\n",
    "print(STATES)\n",
    "for t in range(n_weeks):\n",
    "    print(\"Week %i,:\"%(t+1), [policy[(state, t)] for state in STATES.keys()])\n",
    "    \n",
    "for t in range(n_weeks+1):\n",
    "    print(\"Week %i,:\"%(t+1), [np.round(V[(STATES[state], t)], 2).item() for state in STATES.keys()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 (Value iteration)\n",
    "*(This problem is adapted from Stanford AA203 course)*\n",
    "\n",
    "In this problem, you will implement value iteration to compute the value function for a rescue drone that needs to deliver aid to a goal state while avoiding regions with fire and uncertain wind conditions.\n",
    "\n",
    "The world is represented as an $n \\times n$ grid, i.e., the state space is\n",
    "\n",
    "$$ \\mathcal{S}  := \\lbrace(x_1, x_2) \\in \\mathbb{Z}_+^2 \\mid | x_1, x_2 \\in \\lbrace 0, 1 . . . , n − 1\\rbrace\\rbrace \\cup \\lbrace (\\texttt{None}, \\texttt{None}) \\rbrace, .$$\n",
    "\n",
    "In these coordinates, $(0, 0)$ represents the bottom left corner of the map and $(n−1, n−1)$ represents the top right corner of the map. While $(\\texttt{None}, \\texttt{None})$ is a terminal state. For any non-terminal state, from any location $x = (x_1, x_2) \\in \\mathcal{S}$, the drone has four possible directions it can move in, i.e.,\n",
    "\n",
    "$$ \\mathcal{A}:= \\lbrace \\texttt{up}, \\texttt{down}, \\texttt{left}, \\texttt{right} \\rbrace.$$\n",
    "\n",
    "The corresponding state changes for each action are:\n",
    "- $\\texttt{up}: (x_1, x_2) \\mapsto (x_1, x_2+1)$ \n",
    "- $\\texttt{down}: (x_1, x_2) \\mapsto (x_1, x_2-1)$ \n",
    "- $\\texttt{left}: (x_1, x_2) \\mapsto (x_1-1, x_2)$ \n",
    "- $\\texttt{right}: (x_1, x_2) \\mapsto (x_1+1, x_2)$ \n",
    "\n",
    "There is a storm centered at $x_\\mathrm{eye} \\in \\mathcal{S}$. The storm’s influence is strongest at its center and decays farther from the center according to the equation \n",
    "\n",
    "$$ \\omega(x) = \\exp\\biggl( -\\frac{\\| x - x_\\mathrm{eye}\\|_2^2}{2\\sigma^2}\\biggr)$$\n",
    "\n",
    "Given its current state $x$ and action $a$, the drone’s next state is determined as follows:\n",
    "- With probability $\\omega(x)$, the storm will cause the drone to move in a uniformly random direction.\n",
    "- With probability $1 − \\omega(x)$, the drone will move in the direction specified by the action.\n",
    "- If the resulting movement would cause the drone to leave $\\mathcal{S}$, then it will not move at all. For example, if the drone is on the right boundary of the map, then moving right will do nothing.\n",
    "- If the drone reaches the goal state, then the drone will always transition to a *terminal state* $(\\texttt{None}, \\texttt{None}).$ Once in the terminal state, the drone remains in that state indefinitely regardless of the action taken.\n",
    "\n",
    "The drone's objective is to reach $x_\\mathrm{goal} \\in \\mathcal{S}$. If the drone reaches the goal state, then it receives a reward of $r_\\mathrm{goal}$ (successfully delivers aid), and a reward of $r_\\mathrm{travel}$ otherwise (cost of traveling one unit). Additionally, there are some states where there is a fire. If the drone reaches a state where there is a fire, then it receives a reward of $r_\\mathrm{fire}$ (drone suffers damage). Once the drone is in the terminal state, it receives zero reward (i.e., mission has terminated). The reward of a trajectory in this infinite horizon problem is a discounted sum of the rewards earned in each timestep, with discount factor $\\gamma \\in (0, 1)$.\n",
    "\n",
    "To find the optimal policy to reach the goal state from any starting location, we perform value iteration. Recall that the value iteration repeats the Bellman update until convergence.\n",
    "\n",
    "$$ V(x) \\leftarrow \\max_{a\\in\\mathcal{A}} \\biggl( \\sum_{x^\\prime \\in \\mathcal{S}} p(x, a, x^\\prime) (R(x^\\prime) + \\gamma V(x^\\prime))\\biggr) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Problem set up\n",
    "Below are some helper functions. Some are filled in, others you will you need to fill in yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "import functools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_terminal_state(state):\n",
    "    \"\"\"\n",
    "    Check if the state is a terminal state.\n",
    "    Args:\n",
    "        state: Current state (row, column).\n",
    "    Returns:\n",
    "        True if the state is terminal, False otherwise.\n",
    "    \"\"\"\n",
    "    return state == (None, None)\n",
    "\n",
    "\n",
    "def _state_space(max_rows, max_columns):\n",
    "    return [(i, j) for i in range(max_rows) for j in range(max_columns)] + [(None, None)]\n",
    "\n",
    "\n",
    "def _reward(state, fire_states, goal_states, fire_value, goal_value, travel_value):\n",
    "    \"\"\"\n",
    "    Reward function for the grid world.\n",
    "    Args:\n",
    "        state: Current state (row, column).\n",
    "        fire_states: List or set of fire states.\n",
    "        goal_states: List or set of goal states.\n",
    "        fire_value: Reward value for fire states.\n",
    "        goal_value: Reward value for goal states.\n",
    "    Returns:\n",
    "        Reward value for the current state.\n",
    "    \"\"\"\n",
    "    #### FILL CODE HERE ####\n",
    "    if state in fire_states:\n",
    "        return fire_value\n",
    "    elif state in goal_states:\n",
    "        return goal_value\n",
    "    else:\n",
    "        return travel_value\n",
    "    ########################\n",
    "    \n",
    "def _transition_function(s, a, w=0, \n",
    "                        max_rows=20, # number of rows\n",
    "                        max_columns=20, # number of columns\n",
    "                        goal_states=set([]), \n",
    "                        action_set=[\"down\", \"right\", \"up\", \"left\"]):\n",
    "    \"\"\"\n",
    "    Transition function for the grid world.\n",
    "    Args:\n",
    "        s: Current state (row, column).\n",
    "        a: Action to take.\n",
    "        w: Probability of taking the action.\n",
    "        max_rows: Number of rows in the grid.\n",
    "        max_columns: Number of columns in the\n",
    "            grid.\n",
    "        action_set: List of possible actions.\n",
    "    Returns:\n",
    "        New state after taking the action.\n",
    "    \"\"\"\n",
    "    i,j = s\n",
    "    if is_terminal_state(s) or (s in goal_states):\n",
    "        #print(f\"Transitioning from state {s} to terminal (None, None)\")\n",
    "        return (None, None)\n",
    "    if (np.random.rand(1) < w)[0]: #random movement due to storm\n",
    "        a = np.random.choice(action_set)\n",
    "    if a == \"up\":\n",
    "        return (min(i+1, max_rows-1), j)\n",
    "    if a == \"right\":\n",
    "        return (i, min(j+1, max_columns-1))\n",
    "    if a == \"down\":\n",
    "        return (max(i-1, 0), j)\n",
    "    if a == \"left\":\n",
    "        return (i, max(j-1, 0))\n",
    "\n",
    "    \n",
    "def _compute_omega_probability(state, storm_eye, storm_sigma):\n",
    "    \"\"\"\n",
    "    Computes the probability of a state being affected by a storm.\n",
    "    Args:\n",
    "        state: Current state (row, column).\n",
    "        storm_eye: Center of the storm (row, column).\n",
    "        storm_sigma: Standard deviation of the storm.\n",
    "    Returns:\n",
    "        Probability of the state being affected by the storm.\n",
    "    \"\"\"\n",
    "    if is_terminal_state(state):\n",
    "        return 0\n",
    "    return np.exp(-((state[0] - storm_eye[0])**2 + (state[1] - storm_eye[1])**2) / (2 * storm_sigma**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Problem set up (continued)\n",
    "\n",
    "Below are the problem parameters:\n",
    "- grid size $20 \\times 20$\n",
    "- $x_\\mathrm{eye}=(10,6)$,$\\: \\sigma = 10$\n",
    "- $\\mathcal{S}_\\mathrm{goal} = \\lbrace (19,9) \\rbrace$\n",
    "- $\\mathcal{S}_\\mathrm{fire} = \\lbrace (10,10), (11,10), (10,11), (11,11), (13, 4), (13, 5), (14, 4), (14, 5) \\rbrace$\n",
    "- $\\gamma = 0.95$\n",
    "- $r_\\mathrm{fire} = -200$\n",
    "- $r_\\mathrm{goal} = 100$\n",
    "- $r_\\mathrm{travel} = -1$\n",
    "\n",
    "Also, there are some helper functions. Some are filled in, others you will you need to fill in yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem set up\n",
    "max_rows, max_columns = 20, 20\n",
    "fire_states = set([(10,10), (11,10), (10,11), (11,11), (13, 4), (13, 5), (14, 4), (14, 5)])\n",
    "storm_eye = (10, 6)\n",
    "storm_sigma = 10\n",
    "goal_states = set([(19,9)])\n",
    "gamma = 0.95\n",
    "fire_value = -200\n",
    "goal_value = 100\n",
    "travel_value = -1\n",
    "action_set=[\"down\", \"right\", \"up\", \"left\"]\n",
    "\n",
    "\n",
    "# fix the problem parameters in the functions to avoid passing them every time\n",
    "state_space = functools.partial(_state_space, max_rows=max_rows, max_columns=max_columns)\n",
    "reward = functools.partial(_reward, fire_states=fire_states, goal_states=goal_states, fire_value=fire_value, goal_value=goal_value, travel_value=-1)\n",
    "transition_function = functools.partial(_transition_function, max_rows=max_rows, max_columns=max_columns, goal_states=goal_states, action_set=action_set)\n",
    "compute_omega_probability = functools.partial(_compute_omega_probability, storm_eye=storm_eye, storm_sigma=storm_sigma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_function(state, action, next_state, w, \n",
    "                          action_set=[\"down\", \"right\", \"up\", \"left\"]):\n",
    "    \"\"\"\n",
    "    Computes the probability of transitioning to a next state given the current state and action.\n",
    "    Args:\n",
    "        state: Current state (row, column).\n",
    "        action: Action to take.\n",
    "        next_state: Next state (row, column).\n",
    "        w: Probability of taking random action.\n",
    "        action_set: List of possible actions.\n",
    "    Returns:\n",
    "        Probability of transitioning to the next state.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "    #### FILL CODE HERE ####\n",
    "    # HINT: Our solution takes ~3 lines of code\n",
    "\n",
    "    num_actions = len(action_set)\n",
    "\n",
    "    # Intended deterministic move\n",
    "    intended_next = transition_function(state, action, w=0)\n",
    "    prob = 0\n",
    "\n",
    "    # Add probability of intended move (no storm)\n",
    "    if intended_next == next_state:\n",
    "        prob += (1 - w)\n",
    "\n",
    "    # Add probabilities for all possible random actions (uniformly likely if storm happens)\n",
    "    for random_action in action_set:\n",
    "        random_next = transition_function(state, random_action, w=0)\n",
    "        if random_next == next_state:\n",
    "            prob += w / num_actions\n",
    "    #print(prob)\n",
    "    return prob\n",
    "    #################\n",
    "\n",
    "def get_possible_next_states(state, action_set):\n",
    "    \"\"\"\n",
    "    Returns the set of possible next states given the current state.\n",
    "    Args:\n",
    "        state: Current state (row, column).\n",
    "    Returns:\n",
    "        Set of possible next states.\n",
    "    \"\"\"\n",
    "    return set([transition_function(state, action, w=0) for action in action_set])\n",
    "\n",
    "def bellman_update(value_tuple, gamma, action_set):\n",
    "    \"\"\"\n",
    "    Performs a Bellman update on the value function.\n",
    "    Args:\n",
    "        value_tuple: Current value function. A tuple of (value, value_terminal).\n",
    "        value: Array representing the value at each state in the grid\n",
    "        value_terminal: Value of the terminal state.\n",
    "        gamma: Discount factor.\n",
    "        action_set: List of possible actions.\n",
    "    Returns:\n",
    "        Updated value_tuple and policy as a dictionary.\n",
    "    \"\"\"\n",
    "    #### FILL CODE HERE ####\n",
    "\n",
    "    policy = {} #init policy\n",
    "    value, value_terminal = value_tuple #unpack value_tuple\n",
    "    new_value = value.copy() # duplicate value_tuple, placeholder for next_state iteration\n",
    "\n",
    "    for state in state_space(): #for each state\n",
    "\n",
    "        row, col = state\n",
    "        max_value = float('-inf')  # Start at a minimum value\n",
    "        best_action = None #init best action  \n",
    "        \n",
    "\n",
    "        #iterate through each possible action at state\n",
    "        valid_action_found = False\n",
    "        for action in action_set:\n",
    "            expected_value = 0 # init expected value of possible action\n",
    "\n",
    "\n",
    "            #iterate through probabilities of next state\n",
    "            for next_state in get_possible_next_states(state, action_set):\n",
    "                next_row, next_col = next_state\n",
    "\n",
    "                # This if statement stops code from breaking, prevents passing None values\n",
    "                if next_row is None or next_col is None:\n",
    "                    continue\n",
    "\n",
    "                #print(\"next state\", next_state)\n",
    "                prob = probability_function(state, action, next_state, compute_omega_probability(state))\n",
    "                #print(\"reward_val\", reward(next_state))\n",
    "                #print(\"next row:\", next_row)\n",
    "                \n",
    "                expected_value += prob * (reward(next_state) + gamma * value[next_row, next_col])\n",
    "                #print(\"expected value:\", expected_value)\n",
    "\n",
    "            if expected_value > max_value:\n",
    "                max_value = expected_value\n",
    "                best_action = action\n",
    "                valid_action_found = True\n",
    "            #print(\"max_value\", max_value)\n",
    "\n",
    "        if valid_action_found:\n",
    "            new_value[row, col] = max_value\n",
    "            policy[state] = best_action\n",
    "\n",
    "        else:\n",
    "            new_value[row,col] = round(value[row,col], 2)\n",
    "            policy[state] = None\n",
    "\n",
    "        print(\"new value added\", new_value[row,col])\n",
    "        #print(\"new_value\", new_value)\n",
    "        policy[state] = best_action\n",
    "\n",
    "    #print(value_terminal)\n",
    "    #print(\"difference\", new_value - value)\n",
    "    print(\"new_value\", new_value)\n",
    "    return  (new_value, value_terminal), policy\n",
    "    ########################\n",
    "\n",
    "        \n",
    "def simulate(start_state, policy, num_steps):\n",
    "    \"\"\"\n",
    "    Simulates the agent's trajectory in the grid world.\n",
    "    Args:\n",
    "        start_state: Starting state (row, column).\n",
    "        policy: Policy to follow.\n",
    "        num_steps: Number of steps to simulate.\n",
    "    Returns:\n",
    "        List of states visited during the simulation.\n",
    "    \"\"\"\n",
    "    states = [start_state]\n",
    "    for _ in range(num_steps):\n",
    "        action = policy[start_state]\n",
    "        w = compute_omega_probability(start_state)\n",
    "        next_state = transition_function(start_state, action, w=w)\n",
    "        if is_terminal_state(next_state):\n",
    "            break\n",
    "        start_state = next_state\n",
    "        states.append(start_state)\n",
    "    return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Value iteration\n",
    "With all the building blocks all completed, you are ready to perform value iteration!\n",
    "Below is the value iteration loop, simulating the policy, and corresponding visualization.\n",
    "Run the code and visualize the results and get some intuition into how the value function changes over the iterations.\n",
    "\n",
    "Then explore how the value and policy changes as you change different problem parameters.\n",
    "Share some insights/findings based on your exploration. Do these insights/findings align with your understanding?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the value function\n",
    "V = (np.zeros([max_rows, max_columns]), 0)\n",
    "# keep list of value functions\n",
    "Vs = [V]\n",
    "dV = []\n",
    "num_iterations = 100 # feel free to change this value as needed\n",
    "for _ in range(num_iterations):\n",
    "    # perform Bellman update\n",
    "    V_new, policy = bellman_update(V, gamma, action_set)\n",
    "    # store the new value function\n",
    "    Vs.append(V_new)\n",
    "    dV.append(np.abs(V_new[0] - V[0]).max())\n",
    "\n",
    "    # check for convergence\n",
    "    if np.abs(V_new[0] - V[0]).max() < 1e-3:  #1e-3\n",
    "        print(\"Converged!\")\n",
    "        break\n",
    "    # update the value function\n",
    "    V = V_new\n",
    "    \n",
    "    \n",
    "start_state = (3,9) # pick a starting state\n",
    "num_steps = 200 # feel free to change this value as needed\n",
    "# simulate the trajectory\n",
    "trajectory = simulate(start_state, policy, num_steps)\n",
    "\n",
    "plt.figure(figsize=(4,2))\n",
    "plt.plot(dV)\n",
    "plt.title('Convergence of Value Function')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Max Change in Value Function')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_policy(policy):\n",
    "    for (row, col), action in policy.items():\n",
    "        if row is None or col is None:\n",
    "            continue\n",
    "        if action == \"up\":\n",
    "            plt.text(col + 0.5, row + 0.5, '↑', ha='center', va='center', color='black', fontsize=8)\n",
    "        elif action == \"down\":\n",
    "            plt.text(col + 0.5, row + 0.5, '↓', ha='center', va='center', color='black', fontsize=8)\n",
    "        elif action == \"left\":\n",
    "            plt.text(col + 0.5, row + 0.5, '←', ha='center', va='center', color='black', fontsize=8)\n",
    "        elif action == \"right\":\n",
    "            plt.text(col + 0.5, row + 0.5, '→', ha='center', va='center', color='black', fontsize=8)\n",
    "\n",
    "\n",
    "# compute the storm strength for each state for plotting later\n",
    "storm_strength = np.zeros([max_rows, max_columns])\n",
    "for state in state_space():\n",
    "    if not is_terminal_state(state):\n",
    "        storm_strength[state] = compute_omega_probability(state)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the value function and storm strength\n",
    "@interact(iteration=(0,len(Vs)-1, 1), t=(0,len(trajectory)-1, 1))\n",
    "def plot_value_function(iteration, t):\n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(Vs[iteration][0], origin='lower', extent=[0, max_columns, 0, max_rows], cmap='viridis', interpolation='nearest')\n",
    "    plt.colorbar(label='Value')\n",
    "    plt.title('Value Function')\n",
    "    plt.xlabel('Column')\n",
    "    plt.ylabel('Row')\n",
    "    plt.xticks(ticks=np.arange(0.5, max_columns, 1), labels=np.arange(0, max_columns))\n",
    "    plt.yticks(ticks=np.arange(0.5, max_rows, 1), labels=np.arange(0, max_rows))\n",
    "    plt.scatter(storm_eye[1] + 0.5, storm_eye[0] + 0.5, c='cyan', s=100, label='Storm Eye')\n",
    "    for fire_state in fire_states:\n",
    "        plt.scatter(fire_state[1] + 0.5, fire_state[0] + 0.5, c='red', s=100)\n",
    "    plt.scatter(fire_state[1] + 0.5, fire_state[0] + 0.5, c='red', s=100, label='Fire State')\n",
    "    for goal_state in goal_states:\n",
    "        plt.scatter(goal_state[1] + 0.5, goal_state[0] + 0.5, c='green', s=100, label='Goal State')\n",
    "        \n",
    "    # Overlay the policy\n",
    "    plot_policy(policy)\n",
    "    # Plot the trajectory\n",
    "    trajectory_x = [state[1] + 0.5 for state in trajectory]\n",
    "    trajectory_y = [state[0] + 0.5 for state in trajectory]\n",
    "    plt.plot(trajectory_x, trajectory_y, color='orange', label='Trajectory', linewidth=2)\n",
    "    plt.scatter(trajectory_x[t], trajectory_y[t], color='orange', s=100, label='Current State')\n",
    "    plt.legend(loc=\"lower left\", framealpha=0.6)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(storm_strength, origin='lower', extent=[0, max_columns, 0, max_rows], cmap='viridis', interpolation='nearest')\n",
    "    plt.colorbar(label='Storm Strength')\n",
    "    plt.title('Storm Strength')\n",
    "    plt.xlabel('Column')\n",
    "    plt.ylabel('Row')\n",
    "    plt.xticks(ticks=np.arange(0.5, max_columns, 1), labels=np.arange(0, max_columns))\n",
    "    plt.yticks(ticks=np.arange(0.5, max_rows, 1), labels=np.arange(0, max_rows))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 (Linear Quadratic Regulator)\n",
    "\n",
    "In class, we looked at the (discrete-time) non-time-varying Linear Quadratic Regular problem. Briefly, the goal is to find a sequence of control inputs $\\mathbf{u}=(u_0, u_1,...,u_{N-1})$, for a time horizon of $N$ time steps, that minimizes the (quadratic) cost \n",
    "\n",
    "$$\n",
    "J(\\mathbf{x}, \\mathbf{u}) = \\left[\\sum_{k=0}^{N-1} \\underbrace{\n",
    "x_k^TQx_k + u_k^TRu_k}_{\\text{Running cost}}\\right] + \\underbrace{x_N^TQ_Nx_N}_{\\text{Terminal cost}}\n",
    "$$\n",
    "\n",
    "where $Q = Q^T \\geq 0, k=0,...,T$, $R=R^T >0$, and subject to linear dynamics $x_{k+1} = Ax_k + B u_k$. Noting that there are no constraints on states and controls, aside from the fact that the system must start from the current state $x_0=x_\\mathrm{curr}$ and obey the linear dynamics.\n",
    "If we assume the value function took the form of $V(x,k) = x^TP_kx$, then we can compute the value for any state at any time step $k$ using the following recursion update rule\n",
    "\n",
    "$$\n",
    "P_{k} = Q + A^TP_{k+1}A - A^TP_{k+1}B(R+B^TP_{k+1}B)^{-1}B^TP_{k+1}A, \\qquad \\text{for}\\; k=N-1,...,0,\n",
    "$$\n",
    "\n",
    "and the corresponding optimal gain $K_k$ where $u_k^\\star = K_kx_k$ is given by\n",
    "\n",
    "\\begin{equation}\n",
    "    K_k = -(R+B^TP_{k+1}B)^{-1}B^TP_{k+1}A, \\qquad \\text{for}\\; k=0,...,N-1\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) LQR with cross term\n",
    "\n",
    "Consider a slightly different case where the running cost has a cross term $2x_k^TS_ku_k$. That is,\n",
    "\n",
    "$$\n",
    "\\tilde{J}(\\mathbf{x}, \\mathbf{u}) = \\left[\\sum_{k=0}^{N-1} \n",
    "        x_k^TQx_k + u_k^TRu_k + \\underbrace{2x_k^TSu_k}_{\\text{Cross term}}\\right] + x_N^TQ_Nx_N,\n",
    "$$\n",
    "\n",
    "where $S \\in \\mathbb{R}^{n\\times m}, \\begin{bmatrix} Q & S \\\\ S^T & R\\end{bmatrix}  \\geq 0$ for $k=0,...,N-1$.\n",
    "What is the corresponding update equation for $P_{k}$ and gain $K_k$ with the cross term present? We are still considering a time-invariant case (i.e., $A, B, Q, R, S$ are constants that do not change over time).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Show work here...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Time-varying LQR\n",
    "\n",
    "For standard LQR (i.e., without the cross term introduced in the previos part), what is the corresponding update equation for $P_{k}$ and gain $K_k$ if the dynamics are time-varying? That is, when $x_{k+1} = A_kx_k + B_ku_k$, where $A_k$ and $B_k$ are dependent on $k$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Show work here...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Dynamic programming vs. convex optimization\n",
    "\n",
    "The dynamic programming method we discussed in class is a method of analytically solving a minimization problem in closed form, minimizing the cost $J(\\mathbf{x}, \\mathbf{u})$ subject to the constraint $x_{k+1} = Ax_k + Bu_k$. But since the cost function and constraint set are both convex, we can also use convex optimization methods to solve the same problem numerically.\n",
    "\n",
    "Consider an LQR problem with\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "1 & 0 & 0.1 & 0 \\\\\n",
    "0 & 1 & 0 & 0.1 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{bmatrix},\\quad B = \\begin{bmatrix}\n",
    "0.005 & 0 \\\\\n",
    "0 & 0.005 \\\\\n",
    "0.1 & 0 \\\\\n",
    "0 & 0.1\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "and $R = I, Q = 3I, Q_N = 10I$, and $N = 10$. The initial state is $x_{init} = [1, 2, -0.25, 0.5]$. These data are given in the cell immediately below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem data (given)\n",
    "\n",
    "# These are the result of discretizing 2D double integrator dynamics with zero-order hold and dt = 0.1\n",
    "A = np.array([[1, 0, 0.1, 0],\n",
    "              [0, 1, 0, 0.1],\n",
    "              [0, 0, 1, 0],\n",
    "              [0, 0, 0, 1]])\n",
    "\n",
    "B = np.array([[0.005, 0],\n",
    "              [0, 0.005],\n",
    "              [0.1, 0],\n",
    "              [0, 0.1]])\n",
    "\n",
    "# LQR cost matrices\n",
    "Q = 3 * np.eye(4)\n",
    "R = np.eye(2)\n",
    "Q_N = 10 * np.eye(4)\n",
    "\n",
    "# Time horizon\n",
    "N = 10\n",
    "\n",
    "# Initial state\n",
    "x_init = np.array([1, 2, -0.25, 0.5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)(i) Solution via LQR\n",
    "In the following cell, find the finite-horizon LQR controller for each time step by the iterative LQR process, and simulate the trajectory over the given time horizon with the given initial state. The provided plotting code at the end of the cell will plot the resulting state trajectory and control history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LQR implementation goes in this cell\n",
    "\n",
    "P_matrices = []\n",
    "K_matrices = []\n",
    "\n",
    "x_trajectory_lqr = np.zeros((N,4))\n",
    "u_history_lqr = np.zeros((N-1, 2))\n",
    "\n",
    "\n",
    "###### FILL CODE HERE ######\n",
    "\n",
    "# as a result of your code, the P_matrices list defined above should contain the ten P matrices in increasing order of time,\n",
    "# and the K_matrices list defined above should contain the nine K matrices in increasing order of time.\n",
    "#\n",
    "# x_trajectory_lqr should contain the complete state trajectory (with the k-th row of x_trajectory_lqr containing\n",
    "# the state at time k), and likewise u_history_lqr should contain the complete control history (with the k-th row\n",
    "# of u_history_lqr containing the control at time k).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########################################\n",
    "\n",
    "# Provided plotting code\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(x_trajectory_lqr[:,0], x_trajectory_lqr[:,1])\n",
    "plt.title(\"State trajectory\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.axis(\"equal\")\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(N-1), u_history_lqr[:,0], label=\"u1\")\n",
    "plt.plot(range(N-1), u_history_lqr[:,1], label=\"u2\")\n",
    "plt.legend()\n",
    "plt.title(\"Control vs. k\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"control\")\n",
    "plt.axis(\"equal\")\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)(ii) Solution via convex optimization\n",
    "In the following cell, directly find a state trajectory and control history that solves the same LQR optimization problem using cvx. The provided plotting code at the end of the cell will plot the resulting state trajectory and control history.\n",
    "\n",
    "Hint: you may find the cvxpy function `quad_form` (documented [here](https://www.cvxpy.org/api_reference/cvxpy.atoms.other_atoms.html#quad-form)) useful. Since $Q$ and $R$ are both positive (semi)definite, the quadratic forms $x^TQx$ and $u^TRu$ are convex, but because of subtleties of [the way cvxpy works](https://www.cvxpy.org/tutorial/dcp/index.html), cvxpy does not immediately recognize those expressions as convex in general. `quad_form` provides additional information to cvxpy that allows it to determine the convexity of those expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVX implementation goes in this cell\n",
    "\n",
    "x_trajectory_cvx = np.zeros((N,4))\n",
    "u_history_cvx = np.zeros((N-1, 2))\n",
    "\n",
    "###### FILL CODE HERE ######\n",
    "\n",
    "# As a result of your code, x_trajectory_cvx should contain the complete state trajectory (with the k-th row of x_trajectory_cvx containing\n",
    "# the state at time k), and likewise u_history_cvx should contain the complete control history (with the k-th row of u_history_cvx\n",
    "# containing the control at time k).\n",
    "\n",
    "\n",
    "\n",
    "######################################################\n",
    "\n",
    "# Provided plotting code\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(x_trajectory_cvx[:,0], x_trajectory_cvx[:,1])\n",
    "plt.title(\"State trajectory\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.axis(\"equal\")\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(N-1), u_history_cvx[:,0], label=\"u1\")\n",
    "plt.plot(range(N-1), u_history_cvx[:,1], label=\"u2\")\n",
    "plt.legend()\n",
    "plt.title(\"Control vs. k\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"control\")\n",
    "plt.axis(\"equal\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Open-loop vs. closed-loop\n",
    "\n",
    "You should have gotten the same results from LQR and cvxpy in the previous part. LQR is a method of designing a _closed-loop controller_ that minimizes $J(\\mathbf{x}, \\mathbf{u})$. In contrast, the trajectory optimization with cvxpy is an _open-loop_ method, which designs an entire trajectory in advance and provides a numerical sequence of control inputs to achieve that trajectory; notably, the sequence of control inputs is given as a fixed, independent output of the optimizer, without reference to whatever the state may be at any point along the trajectory.\n",
    "\n",
    "When and why might we prefer one technique over the other? Discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. Run the cell below to check that the solutions found by both methods match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"States match:\", np.allclose(x_trajectory_cvx, x_trajectory_lqr))\n",
    "print(\"Controls match:\", np.allclose(u_history_cvx, u_history_lqr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4 (Trajectory optimization - sequential quadratic programming)\n",
    "\n",
    "In this problem, you will explore sequential quadratic programming, an algorithm where you successively convexify your nonlinear trajectory optmization problem about a previous solution, and reduce the problem into a quadratic program.\n",
    "Assuming the cost objective is already quadratic, the bulk of the convexification will be focused on linearizing the dynamics and constraints about a previous solution.\n",
    "\n",
    "Below is an implementation of SQP with a dynamically extended simple car model avoiding *one* circular obstacle. You should read each line of the code; the following questions will be based on your understanding of the SQP algorithm and following implementation.\n",
    "\n",
    "The dynamically extended simple car has the following dynamics:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\dot{x} \\\\ \\dot{y} \\\\ \\dot{\\theta} \\\\ \\dot{v}\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "v\\cos\\theta\\\\ v\\sin\\theta \\\\ \\frac{v}{L}\\tilde{\\delta} \\\\ a\n",
    "\\end{bmatrix}, \\qquad u = \\begin{bmatrix} \\tilde{\\delta} \\\\ a \\end{bmatrix} \\: \\text{where} \\: \\tilde{\\delta} = \\tan\\delta\n",
    "$$\n",
    "\n",
    "Note: We have made the substitution $\\tilde{\\delta} = \\tan\\delta$ so that the system is control affine since the mapping is bijective over $(-\\pi/2, \\pi/2)$\n",
    "\n",
    "Let $g(x; x_\\mathrm{ob}, r)$ denote a function that measure how far away the state $x$ is from a circular obstacle centered at $x_\\mathrm{ob}$ with radius $r$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to install a new package `dynamaxsys`.\n",
    "You can install it by running \n",
    "```\n",
    "pip install dynamaxsys==0.0.3\n",
    "```\n",
    "\n",
    "Alternatively, you can clone it locally by following the instructions here `https://github.com/UW-CTRL/dynamaxsys.git`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cvxpy as cp # import cvxpy\n",
    "\n",
    "# in this problem, we will use the dynamaxsys library to import dynamical systems implemented in JAX: https://github.com/UW-CTRL/dynamaxsys\n",
    "from dynamaxsys.simplecar import DynamicallyExtendedSimpleCar\n",
    "from dynamaxsys.base import get_discrete_time_dynamics\n",
    "from dynamaxsys.utils import linearize\n",
    "\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import functools\n",
    "import functools\n",
    "from ipywidgets import interact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the robot dynamics\n",
    "wheelbase = 1.0\n",
    "dt = 0.1\n",
    "ct_robot_dynamics = DynamicallyExtendedSimpleCar(wheelbase=wheelbase) # robot dynamics\n",
    "dt_robot_dynamics = get_discrete_time_dynamics(ct_robot_dynamics, dt=dt) # discrete time dynamics\n",
    "state_dim = dt_robot_dynamics.state_dim\n",
    "control_dim = dt_robot_dynamics.control_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some helper functions\n",
    "\n",
    "# define obstacle function g(x) >= 0\n",
    "# where g(x) is the distance from the obstacle\n",
    "@jax.jit\n",
    "def obstacle_constraint(state, obstacle, radius):\n",
    "    return jnp.linalg.norm(state[:2] - obstacle[:2]) - radius\n",
    "\n",
    "# function to simulate the discrete time dynamics given initial state and control sequence\n",
    "@functools.partial(jax.jit, static_argnames=[\"dt_dynamics\"])\n",
    "def simulate_discrete_time_dynamics(dt_dynamics, state, controls, t0, dt):\n",
    "    states = [state]\n",
    "    t = t0\n",
    "    for c in controls:\n",
    "        state = dt_dynamics(state, c, t)\n",
    "        states.append(state)\n",
    "        t += dt\n",
    "    return jnp.stack(states)\n",
    "\n",
    "# jit the linearize constraint functions to make it run faster\n",
    "linearize_obstacle = jax.jit(jax.vmap(jax.grad(obstacle_constraint), in_axes=[0, None, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the problem parameters\n",
    "planning_horizon = 25 # length of the planning horizon\n",
    "num_time_steps = 50 # number of time steps to simulate\n",
    "num_sqp_iterations = 15 # number of SQP iterations\n",
    "t = 0. # this doesn't affect anything, but a value is needed \n",
    "\n",
    "# control and velocity limits\n",
    "v_max = 1.5\n",
    "v_min = 0.\n",
    "acceleration_max = 1.0\n",
    "acceleration_min = -1.0\n",
    "steering_max = 0.5\n",
    "steering_min = -0.5\n",
    "\n",
    "# obstacle parameters\n",
    "obstacle_location = jnp.array([1.0, 0.0]) # obstacle location\n",
    "obstacle_radius = 0.5 # obstacle radius\n",
    "robot_radius = 0.1 # robot radius\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) The inner quadratic program problem\n",
    "\n",
    "Take a close look at the following two cells. Let $x^\\mathrm{prev}_t$ and $u^\\mathrm{prev}_t$ denote the state and control at time $t$ from the *previous* SQP iteration.\n",
    "Write out the exact quadratic program that is being solved at each SQP iteration. Keep variables/parameters in terms of their names and don't use their numerical values. For example, use $\\beta_1$ instead of 0.2.\n",
    "\n",
    "\n",
    "Additionally, describe what each term in the problem represents, and the expression for them. That is, define mathematically what they are using mathematical expressions, their role within the optimization problem, and describe in words the interpretation of them. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the QP here....\n",
    "\n",
    "$$ ... $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up cvxpy problem variables and parameters\n",
    "xs = cp.Variable([planning_horizon+1, state_dim])  # cvx variable for states\n",
    "us = cp.Variable([planning_horizon, control_dim])  # cvx variable for controls\n",
    "slack = cp.Variable(1) # slack variable to make sure the problem is feasible\n",
    "As = [cp.Parameter([state_dim, state_dim]) for _ in range(planning_horizon)]  # parameters for linearized dynamics\n",
    "Bs = [cp.Parameter([state_dim, control_dim]) for _ in range(planning_horizon)] # parameters for linearized dynamics\n",
    "Cs = [cp.Parameter([state_dim]) for _ in range(planning_horizon)] # parameters for linearized dynamics\n",
    "\n",
    "Gs = [cp.Parameter([state_dim]) for _ in range(planning_horizon+1)] # parameters for linearized constraints\n",
    "hs = [cp.Parameter(1) for _ in range(planning_horizon+1)] # parameters for linearized constraints\n",
    "\n",
    "xs_previous = cp.Parameter([planning_horizon+1, state_dim]) # parameter for previous solution\n",
    "us_previous = cp.Parameter([planning_horizon, control_dim]) # parameter for previous solution\n",
    "initial_state = cp.Parameter([state_dim]) # parameter for current robot state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up cvxpy problem cost and constraints\n",
    "beta1 = 0.2 # coefficient for control effort\n",
    "beta2 = 5. # coefficient for progress\n",
    "beta3 = 10. # coefficient for trust region\n",
    "slack_penalty = 1000. # coefficient for slack variable\n",
    "markup = 1.05\n",
    "\n",
    "objective = beta2 * (xs[-1,2]**2 + xs[-1,1]**2 - xs[-1,0]) + beta3 * (cp.sum_squares(xs - xs_previous) + cp.sum_squares(us - us_previous)) + slack_penalty * slack**2\n",
    "constraints = [xs[0] == initial_state, slack >= 0] # initial state and slack constraint\n",
    "for t in range(planning_horizon):\n",
    "    objective += (beta1 * cp.sum_squares(us[t]) + beta1 * (xs[t,2]**2 + xs[t,1]**2 - xs[t,0]) ) * markup**t\n",
    "    constraints += [xs[t+1] == As[t] @ xs[t] + Bs[t] @ us[t] + Cs[t]] # dynamics constraint\n",
    "    constraints += [xs[t,-1] <= v_max, xs[t,-1] >= v_min, us[t,1] <= acceleration_max, us[t,1] >= acceleration_min, us[t,0] <= steering_max, us[t,0] >= steering_min] # control and velocity limit constraints\n",
    "    constraints += [Gs[t] @ xs[t] + hs[t] >= -slack] # linearized collision avoidance constraint\n",
    "constraints += [xs[planning_horizon,-1] <= v_max, xs[planning_horizon,-1] >= v_min, Gs[planning_horizon] @ xs[planning_horizon] + hs[planning_horizon] >= -slack] # constraints for last planning horizon step\n",
    "prob = cp.Problem(cp.Minimize(objective), constraints) # construct problem\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial states\n",
    "robot_state = jnp.array([-1.5, -0.1, 0., 1.])  # robot starting state\n",
    "robot_trajectory = [robot_state] # list to collect robot's state as it replans\n",
    "sqp_list = [] # list to collect each sqp iteration \n",
    "robot_control_list = []  # list to collect robot's constrols as it replans\n",
    "robot_trajectory_list = [] # list to collect robot's planned trajectories\n",
    "\n",
    "# initial robot planned state and controls\n",
    "previous_controls = jnp.zeros([planning_horizon, control_dim]) # initial guess for robot controls\n",
    "previous_states =  simulate_discrete_time_dynamics(dt_robot_dynamics, robot_state, previous_controls, 0., dt) # initial guess for robot states\n",
    "xs_previous.value = np.array(previous_states) # set xs_previous parameter value\n",
    "us_previous.value = np.array(previous_controls) # set us_previous parameter value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) The planning loop\n",
    "\n",
    "In the follow cell is the main loop where the planning at each time step occurs. At each time step, multiple SQP iterations are performed (either until convergence or for some fixed number of iterations).\n",
    "The cell is intentionally uncommented. Please add comments to each line of code, giving a brief description of the purpose/function of each line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ADD COMMENTS TO EXPLAIN THE SQP SOLVER #####\n",
    "\n",
    "solver = cp.CLARABEL\n",
    "\n",
    "for t in range(num_time_steps):\n",
    "    initial_state.value = np.array(robot_state)\n",
    "    sqp_solutions = [previous_states]\n",
    "    \n",
    "    for i in range(num_sqp_iterations):\n",
    "        As_value, Bs_value, Cs_value = jax.vmap(linearize, in_axes=[None, 0, 0, None])(dt_robot_dynamics, previous_states[:-1], previous_controls, 0.)\n",
    "        Gs_value = linearize_obstacle(previous_states, obstacle_location, obstacle_radius + robot_radius) \n",
    "        hs_value = jax.vmap(obstacle_constraint, [0, None, None])(previous_states, obstacle_location, obstacle_radius + robot_radius) - jax.vmap(jnp.dot, [0, 0])(Gs_value, previous_states)\n",
    "\n",
    "        for i in range(planning_horizon):\n",
    "            As[i].value = np.array(As_value[i])\n",
    "            Bs[i].value = np.array(Bs_value[i])\n",
    "            Cs[i].value = np.array(Cs_value[i])\n",
    "            Gs[i].value = np.array(Gs_value[i])\n",
    "            hs[i].value = np.array(hs_value[i:i+1])\n",
    "        Gs[planning_horizon].value = np.array(Gs_value[planning_horizon])\n",
    "        hs[planning_horizon].value = np.array(hs_value[planning_horizon:planning_horizon+1])\n",
    "        \n",
    "        result = prob.solve(solver=solver)\n",
    "\n",
    "        previous_controls = us.value\n",
    "        previous_states =  simulate_discrete_time_dynamics(dt_robot_dynamics, robot_state, previous_controls, 0., dt)\n",
    "        sqp_solutions.append(previous_states)\n",
    "        xs_previous.value = np.array(previous_states)\n",
    "        us_previous.value = np.array(previous_controls)\n",
    "    sqp_list.append(np.stack(sqp_solutions))\n",
    "    robot_control = previous_controls[0]\n",
    "    robot_control_list.append(robot_control)\n",
    "    robot_state = dt_robot_dynamics(robot_state, robot_control, 0.)\n",
    "    robot_trajectory.append(robot_state)\n",
    "    robot_trajectory_list.append(previous_states)\n",
    "    previous_states =  simulate_discrete_time_dynamics(dt_robot_dynamics, robot_state, previous_controls, 0., dt)\n",
    "    \n",
    "    \n",
    "robot_trajectory = jnp.stack(robot_trajectory)\n",
    "robot_controls = jnp.stack(robot_control_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the results. No need to add comments here. Just run this cell to visualize the results\n",
    "@interact(i=(0,num_time_steps-1), j=(0,num_sqp_iterations-1))\n",
    "def plot(i, j):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 4), gridspec_kw={'width_ratios': [2, 1]})\n",
    "    # fig, axs = plt.subplots(1,2, figsize=(10, 4))\n",
    "    ax = axs[0]\n",
    "    robot_position = robot_trajectory[i, :2]\n",
    "    circle1 = plt.Circle(robot_position, robot_radius, color='C0', alpha=0.4)\n",
    "    circle2 = plt.Circle(obstacle_location, obstacle_radius, color='C1', alpha=0.4)\n",
    "    ax.add_patch(circle1)\n",
    "    ax.add_patch(circle2)\n",
    "    ax.plot(robot_trajectory[:,0], robot_trajectory[:,1], \"o-\", markersize=3, color='black')\n",
    "    ax.plot(robot_trajectory_list[i][:,0], robot_trajectory_list[i][:,1], \"o-\", markersize=3, color='red', label=\"planned\")\n",
    "    # Plot planned trajectory for the selected SQP iteration\n",
    "    planned_trajectory = sqp_list[i][j]\n",
    "    ax.plot(planned_trajectory[:, 0], planned_trajectory[:, 1], \"o-\", markersize=3, color='green', alpha=0.4, label=\"Planned Trajectory\")\n",
    "    ax.scatter(robot_trajectory[i:i+1,0], robot_trajectory[i:i+1,1], s=30,  color='C0', label=\"Robot\")\n",
    "    ax.set_xlim([-2, 7])\n",
    "    ax.grid()\n",
    "    ax.legend()\n",
    "    ax.axis(\"equal\")\n",
    "\n",
    "    ax.set_title(\"heading=%.2f velocity=%.2f\"%(robot_trajectory[i,2], robot_trajectory[i,3]))\n",
    "    \n",
    "    ax = axs[1]\n",
    "    plt.plot(robot_controls)\n",
    "    plt.scatter([i], robot_controls[i:i+1, 0], label=\"$tan(\\\\delta)$\", color='C0')\n",
    "    plt.scatter([i], robot_controls[i:i+1, 1], label=\"Acceleration\", color='C1')\n",
    "\n",
    "    plt.hlines(steering_min, 0, num_time_steps-1, color='C0', linestyle='--')\n",
    "    plt.hlines(steering_max, 0, num_time_steps-1, color='C0', linestyle='--')\n",
    "    plt.hlines(acceleration_min, 0, num_time_steps-1, color='C1', linestyle='--')\n",
    "    plt.hlines(acceleration_max, 0, num_time_steps-1, color='C1', linestyle='--')\n",
    "    \n",
    "    plt.plot(robot_trajectory[:,-1], markersize=3, color='C2')\n",
    "    plt.scatter([i], robot_trajectory[i:i+1, 3], label=\"Velocity\", color='C2')\n",
    "    plt.hlines(v_min, 0, num_time_steps-1, color='C2', linestyle='--')\n",
    "    plt.hlines(v_max, 0, num_time_steps-1, color='C2', linestyle='--')\n",
    "    ax.set_xlim([0, num_time_steps])\n",
    "    ax.set_ylim([-2, 2])\n",
    "    ax.set_xlabel(\"Time step\")\n",
    "    ax.set_ylabel(\"Control\")\n",
    "    ax.set_title(\"Velocity, steering and acceleration\")\n",
    "    ax.legend()\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Add another obstacle\n",
    "Great! You have just planned a robot trajectory using the SQP algorithm! \n",
    "After parsing through all the code, perhaps you could have written that up yourself from scratch right?\n",
    "Now, edit the code above to add another obstacle of the same size centered at $(3, -0.5)$ and amend your answer to (a) to include the second obstacle.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Try out different parameter values\n",
    "Try out different parameter values, cost functions, obstacle size/locations, etc and see what kind of behaviors emerge. \n",
    "As you investigate, think about things like \"Are there instances where the solution isn't very good? Why is that so?\" or \"How much does initial guess matter?\" or \"Are the obstable constraints always satisfied?\"\n",
    "You can come up with your own questions to guide your exploration.\n",
    "\n",
    "Share your findings/insights based on your exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
