{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tynan 548 Project File\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP X \n",
    "What this step does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cvxpy as cp # import cvxpy\n",
    "\n",
    "# in this problem, we will use the dynamaxsys library to import dynamical systems implemented in JAX: https://github.com/UW-CTRL/dynamaxsys\n",
    "from dynamaxsys.simplecar import DynamicallyExtendedSimpleCar\n",
    "from dynamaxsys.base import get_discrete_time_dynamics\n",
    "from dynamaxsys.utils import linearize\n",
    "\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import functools\n",
    "from ipywidgets import interact\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following segment involves the rover going from the starting point to the first sampling location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the robot dynamics\n",
    "wheelbase = 1.0\n",
    "dt = 0.1\n",
    "ct_robot_dynamics = DynamicallyExtendedSimpleCar(wheelbase=wheelbase) # robot dynamics\n",
    "dt_robot_dynamics = get_discrete_time_dynamics(ct_robot_dynamics, dt=dt) # discrete time dynamics\n",
    "state_dim = dt_robot_dynamics.state_dim\n",
    "control_dim = dt_robot_dynamics.control_dim\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the problem parameters\n",
    "planning_horizon = 20 # length of the planning horizon\n",
    "num_time_steps = 300 # number of time steps to simulate\n",
    "num_sqp_iterations = 15 # number of SQP iterations\n",
    "t = 0. # this doesn't affect anything, but a value is needed \n",
    "\n",
    "# control and velocity limits\n",
    "v_max = 2.5 #2\n",
    "v_min = 0.2\n",
    "acceleration_max = 2.0 #2/-2\n",
    "acceleration_min = -2.0\n",
    "steering_max = 1. #started 0.5/-0.5\n",
    "steering_min = -1.\n",
    "\n",
    "# obstacle parameters\n",
    "obstacle_location = jnp.array([1.0, 1.0]) # obstacle location\n",
    "obstacle_location2 = jnp.array([3.0, -0.5]) # obstacle location\n",
    "obstacle_radius = 0.5 # obstacle radius\n",
    "robot_radius = 0.1 # robot radius\n",
    "goal_locations = [jnp.array([3.5, 3.0]), jnp.array([3.5, 1.0]), jnp.array([5.5, 1.0]), jnp.array([2.0, -2.5]), jnp.array([0.0, 0.0]), jnp.array([2.0, 6.0]), jnp.array([-5.0, 6.0]), jnp.array([5.0, -4.0])]\n",
    "goal_location = goal_locations[0]\n",
    "goal_radius = 0.25\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Establish Rectangle Class ## \n",
    "\n",
    "class Rectangle:\n",
    "    def __init__(self, center, length, width, buffer=0.4):\n",
    "        \"\"\"\n",
    "        Initialize a rectangular obstacle.\n",
    "\n",
    "        Args:\n",
    "            center (tuple or jnp.ndarray): (x, y) coordinates of the center.\n",
    "            length (float): Length along the x-axis.\n",
    "            width (float): Width along the y-axis.\n",
    "            buffer (float): Optional buffer space for inflation.\n",
    "        \"\"\"\n",
    "        self.center = jnp.array(center)\n",
    "        self.length = length\n",
    "        self.width = width\n",
    "        self.buffer = buffer\n",
    "\n",
    "    def inflated_half_lengths(self):\n",
    "        return (self.length / 2 + self.buffer, self.width / 2 + self.buffer)\n",
    "\n",
    "    def as_array(self):\n",
    "        \"\"\"Returns center and dimensions as a single array.\"\"\"\n",
    "        return jnp.array([*self.center, self.length, self.width, self.buffer])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Rectangle(center={self.center.tolist()}, l={self.length}, w={self.width}, buffer={self.buffer})\"\n",
    "\n",
    "\n",
    "rectangles = [\n",
    "    Rectangle(center=(2.0, 1.0), length=0.5, width=5.0),\n",
    "    Rectangle(center=(6.0, -1.0), length=1.0, width=1.0),\n",
    "    Rectangle(center=(5.8, 2.8), length=0.5, width=0.5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS #\n",
    "\n",
    "# define obstacle function g(x) >= 0\n",
    "# where g(x) is the distance from the obstacle\n",
    "@jax.jit\n",
    "def obstacle_constraint(state, obstacle, radius):\n",
    "    return jnp.linalg.norm(state[:2] - obstacle[:2]) - radius\n",
    "\n",
    "\n",
    "def smooth_max(a, b, alpha=10.0):\n",
    "    return (1/alpha) * jnp.log(jnp.exp(alpha * a) + jnp.exp(alpha * b))\n",
    "\n",
    "@jax.jit\n",
    "def rect_constraint_smooth(state, obstacle, l, w, buffer, alpha=10.0):\n",
    "    dx = jnp.abs(state[0] - obstacle[0]) - l / 2 - buffer\n",
    "    dy = jnp.abs(state[1] - obstacle[1]) - w / 2 - buffer\n",
    "    return smooth_max(dx, dy, alpha=alpha)\n",
    "\n",
    "\n",
    "# define goal function g(x) >= 0\n",
    "# where g(x) is the distance from the goal\n",
    "\n",
    "@jax.jit\n",
    "def goal_constraint(state, goal, radius):\n",
    "    return jnp.linalg.norm(state[:2] - goal[:2]) - radius\n",
    "\n",
    "\n",
    "# function to simulate the discrete time dynamics given initial state and control sequence\n",
    "@functools.partial(jax.jit, static_argnames=[\"dt_dynamics\"])\n",
    "def simulate_discrete_time_dynamics(dt_dynamics, state, controls, t0, dt):\n",
    "    states = [state]\n",
    "    t = t0\n",
    "    for c in controls:\n",
    "\n",
    "        state = dt_dynamics(state, c, t)\n",
    "        states.append(state)\n",
    "        t += dt\n",
    "    return jnp.stack(states)\n",
    "\n",
    "# function to simulate the discrete time dynamics given initial state and control sequence\n",
    "# function slightly modified to add noise \n",
    "@functools.partial(jax.jit, static_argnames=[\"dt_dynamics\"])\n",
    "def simulate_discrete_time_dynamics_with_noise(dt_dynamics, state, controls, t0, dt, noises):\n",
    "    states = [state]\n",
    "    t = t0\n",
    "    for (c,noise) in zip(controls, noises):\n",
    "        state = dt_robot_dynamics(state, c, t + noise * dt) # take out noises for now\n",
    "        states.append(state)\n",
    "        t += dt\n",
    "    return jnp.stack(states, -1)\n",
    "\n",
    "# jit the linearize constraint functions to make it run faster\n",
    "linearize_obstacle = jax.jit(jax.vmap(jax.grad(obstacle_constraint), in_axes=[0, None, None]))\n",
    "linearize_goal = jax.jit(jax.vmap(jax.grad(goal_constraint), in_axes=[0, None, None]))\n",
    "linearize_rect_smooth = jax.jit(\n",
    "    jax.vmap(jax.grad(rect_constraint_smooth, argnums=0), in_axes=[0, None, None, None, None])\n",
    ")\n",
    "\n",
    "states = jnp.array([\n",
    "    [1.0, 1.0],\n",
    "    [2.0, 3.0],\n",
    "    [0.0, 0.0]\n",
    "])\n",
    "\n",
    "#rectangle = jnp.array([2.0, 1])\n",
    "#l = .5\n",
    "#w = 4\n",
    "#alpha = 5.\n",
    "#buffer = .2\n",
    "\n",
    "#grads = linearize_rect_smooth(states, rectangle, l, w, alpha)\n",
    "#print(grads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up cvxpy problem variables and parameters\n",
    "xs = cp.Variable([planning_horizon+1, state_dim])  # cvx variable for states\n",
    "us = cp.Variable([planning_horizon, control_dim])  # cvx variable for controls\n",
    "slack = cp.Variable(1) # slack variable to make sure the problem is feasible\n",
    "As = [cp.Parameter([state_dim, state_dim]) for _ in range(planning_horizon)]  # parameters for linearized dynamics\n",
    "Bs = [cp.Parameter([state_dim, control_dim]) for _ in range(planning_horizon)] # parameters for linearized dynamics\n",
    "Cs = [cp.Parameter([state_dim]) for _ in range(planning_horizon)] # parameters for linearized dynamics\n",
    "\n",
    "Gs = [cp.Parameter([state_dim]) for _ in range(planning_horizon+1)] # parameters for linearized constraints\n",
    "hs = [cp.Parameter(1) for _ in range(planning_horizon+1)] # parameters for linearized constraints\n",
    "#Gs_rect1 = [cp.Parameter([state_dim]) for _ in range(planning_horizon+1)] # parameters for linearized constraints\n",
    "#hs_rect1 = [cp.Parameter(1) for _ in range(planning_horizon+1)] # parameters for linearized constraints\n",
    "\n",
    "Gs_rects = [[cp.Parameter([state_dim]) for _ in range(planning_horizon + 1)] for _ in range(len(rectangles))]\n",
    "hs_rects = [[cp.Parameter(1) for _ in range(planning_horizon + 1)] for _ in range(len(rectangles))]\n",
    "\n",
    "\n",
    "Goal_Gs = [cp.Parameter([state_dim]) for _ in range(planning_horizon+1)] # parameters for linearized constraints\n",
    "Goal_hs = [cp.Parameter(1) for _ in range(planning_horizon+1)] # parameters for linearized constraints\n",
    "\n",
    "xs_previous = cp.Parameter([planning_horizon+1, state_dim]) # parameter for previous solution\n",
    "us_previous = cp.Parameter([planning_horizon, control_dim]) # parameter for previous solution\n",
    "initial_state = cp.Parameter([state_dim]) # parameter for current robot state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up cvxpy problem cost and constraints\n",
    "def construct_problem(goal_location):\n",
    "    beta1 = 0.2 # coefficient for control effort\n",
    "    beta2 = 5. # coefficient for progress\n",
    "    beta3 = 10. # coefficient for trust region\n",
    "    slack_penalty = 1000. # coefficient for slack variable\n",
    "    markup = 1.0\n",
    "    goal_cost_weight = 10.\n",
    "    terminal_goal_weight = 1.\n",
    "    forward_velocity_weight = 0.1 # encourage robot to keep moving forward\n",
    "    low_velocity_penalty_weight = 1\n",
    "\n",
    "    ## removed objective beta2 * (xs[-1,2]**2 + xs[-1,1]**2 - xs[-1,0])\n",
    "\n",
    "    # Discourages large changes in state and control\n",
    "    objective = beta3 * (cp.sum_squares(xs - xs_previous) + cp.sum_squares(us - us_previous)) + slack_penalty * slack**2\n",
    "\n",
    "\n",
    "    constraints = [xs[0] == initial_state, slack >= 0] # initial state and slack constraint\n",
    "    for t in range(planning_horizon):\n",
    "        objective += (beta1 * cp.sum_squares(us[t]) ) * markup**t # only penalizes control effort\n",
    "        #terminal_goal_dist_sq = cp.sum_squares(xs[planning_horizon, :2] - goal_location) * markup**t\n",
    "        #objective += terminal_goal_weight * terminal_goal_dist_sq\n",
    "\n",
    "            # NEW: Encourage steady progress toward the goal\n",
    "        step_goal_dist_sq = cp.sum_squares(xs[t, :2] - goal_location)\n",
    "        objective += goal_cost_weight * step_goal_dist_sq * markup**t\n",
    "        objective += -forward_velocity_weight * cp.sum(xs[:, 2])  # Encourages movement\n",
    "\n",
    "        constraints += [xs[t+1] == As[t] @ xs[t] + Bs[t] @ us[t] + Cs[t]] # dynamics constraint\n",
    "        constraints += [xs[t,-1] <= v_max, xs[t,-1] >= v_min, us[t,1] <= acceleration_max, us[t,1] >= acceleration_min, us[t,0] <= steering_max, us[t,0] >= steering_min] # control and velocity limit constraints\n",
    "        constraints += [Gs[t] @ xs[t] + hs[t] >= -slack] # linearized collision avoidance constraint for obstacles\n",
    "        #constraints += [Gs_rect1[t] @ xs[t] + hs_rect1[t] >= 0]\n",
    "        for r_idx in range(len(Gs_rects)):\n",
    "            constraints += [Gs_rects[r_idx][t] @ xs[t] + hs_rects[r_idx][t] >= 0]\n",
    "\n",
    "    #constraints += [xs[planning_horizon,-1] <= v_max, xs[planning_horizon,-1] >= v_min, Gs[planning_horizon] @ xs[planning_horizon] + hs[planning_horizon] >= -slack, Gs_rect1[planning_horizon] @ xs[planning_horizon] + hs_rect1[planning_horizon] >= -slack] # constraints for last planning horizon step\n",
    "    constraints += [xs[planning_horizon,-1] <= v_max, xs[planning_horizon,-1] >= v_min,\n",
    "                Gs[planning_horizon] @ xs[planning_horizon] + hs[planning_horizon] >= -slack]\n",
    "\n",
    "    for r_idx in range(len(Gs_rects)):\n",
    "        constraints += [Gs_rects[r_idx][planning_horizon] @ xs[planning_horizon] + hs_rects[r_idx][planning_horizon] >= -slack]\n",
    "\n",
    "    return cp.Problem(cp.Minimize(objective), constraints) # construct problem\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial states\n",
    "robot_state = jnp.array([-1.5, -0.5, 0., 1.])  # robot starting state\n",
    "robot_trajectory = [robot_state] # list to collect robot's state as it replans\n",
    "sqp_list = [] # list to collect each sqp iteration \n",
    "robot_control_list = []  # list to collect robot's constrols as it replans\n",
    "robot_trajectory_list = [] # list to collect robot's planned trajectories\n",
    "\n",
    "# initial robot planned state and controls\n",
    "previous_controls = jnp.zeros([planning_horizon, control_dim]) # initial guess for robot controls\n",
    "previous_states =  simulate_discrete_time_dynamics(dt_robot_dynamics, robot_state, previous_controls, 0., dt) # initial guess for robot states\n",
    "xs_previous.value = np.array(previous_states) # set xs_previous parameter value\n",
    "us_previous.value = np.array(previous_controls) # set us_previous parameter value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Replan at each timestep!\n",
    "Hopefully you saw from above the need to replan at each time step.\n",
    "Run the following cells and answer the question at the end.\n",
    "\n",
    "(Just run the cell below)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial states\n",
    "robot_state = jnp.array([-1.5, -0.5, 0., 1.])  # robot starting state\n",
    "robot_trajectory = [robot_state] # list to collect robot's state as it replans\n",
    "sqp_list = [] # list to collect each sqp iteration \n",
    "robot_control_list = []  # list to collect robot's constrols as it replans\n",
    "robot_trajectory_list = [] # list to collect robot's planned trajectories\n",
    "\n",
    "# initial robot planned state and controls\n",
    "previous_controls = jnp.zeros([planning_horizon, control_dim]) # initial guess for robot controls\n",
    "previous_states =  simulate_discrete_time_dynamics(dt_robot_dynamics, robot_state, previous_controls, 0., dt) # initial guess for robot states\n",
    "xs_previous.value = np.array(previous_states) # set xs_previous parameter value\n",
    "us_previous.value = np.array(previous_controls) # set us_previous parameter value \n",
    "\n",
    "# precompute the noise\n",
    "key = jax.random.PRNGKey(0)\n",
    "noise_covar = jnp.diag(jnp.array([0.1, 0.1, 0.05, 0.2])) # noise covariance\n",
    "noises = jax.random.multivariate_normal(key, jnp.zeros(robot_state.shape), noise_covar, shape=(num_time_steps,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun the simulation, replanning at each time step, and adding some noise when computing the robot's next state.\n",
    "\n",
    "(Just run the cell below)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No solution found\n",
      "No solution found\n",
      "Goal switched to: [3.5 1. ]\n",
      "Goal switched to: [5.5 1. ]\n",
      "Goal switched to: [ 2.  -2.5]\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "No solution found\n",
      "Goal switched to: [0. 0.]\n",
      "Goal switched to: [2. 6.]\n",
      "Goal switched to: [-5.  6.]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Parameter value must be real.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[437], line 72\u001b[0m\n\u001b[0;32m     69\u001b[0m Goal_hs[planning_horizon]\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(Goal_hs_value[planning_horizon:planning_horizon\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rectangles)):\n\u001b[1;32m---> 72\u001b[0m     Gs_rects[r_idx][planning_horizon]\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(Gs_rects_values[r_idx][planning_horizon])\n\u001b[0;32m     73\u001b[0m     hs_rects[r_idx][planning_horizon]\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(hs_rects_values[r_idx][planning_horizon:planning_horizon\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     77\u001b[0m result \u001b[38;5;241m=\u001b[39m prob\u001b[38;5;241m.\u001b[39msolve(solver\u001b[38;5;241m=\u001b[39msolver)\n",
      "File \u001b[1;32mc:\\Users\\ariel\\anaconda3\\envs\\ME548\\Lib\\site-packages\\cvxpy\\expressions\\leaf.py:465\u001b[0m, in \u001b[0;36mLeaf.value\u001b[1;34m(self, val)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    463\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWriting to a sparse CVXPY expression via `.value` is discouraged.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    464\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Use `.value_sparse` instead\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 465\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_value(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_value(val))\n",
      "File \u001b[1;32mc:\\Users\\ariel\\anaconda3\\envs\\ME548\\Lib\\site-packages\\cvxpy\\expressions\\leaf.py:580\u001b[0m, in \u001b[0;36mLeaf._validate_value\u001b[1;34m(self, val, sparse_path)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    579\u001b[0m             attr_str \u001b[38;5;241m=\u001b[39m ([k \u001b[38;5;28;01mfor\u001b[39;00m (k, v) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreal\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 580\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    581\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m value must be \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, attr_str)\n\u001b[0;32m    582\u001b[0m         )\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m val\n",
      "\u001b[1;31mValueError\u001b[0m: Parameter value must be real."
     ]
    }
   ],
   "source": [
    "prob = construct_problem(goal_location)\n",
    "solver = cp.CLARABEL\n",
    "goal_index = 0\n",
    "for t in range(num_time_steps):\n",
    "    \n",
    "    goal_threshold = 0.2  # adjust threshold as needed\n",
    "    distance_to_goal = np.linalg.norm(robot_state[:2] - goal_location)\n",
    "    if distance_to_goal < goal_threshold:\n",
    "        goal_location = goal_locations[goal_index + 1]\n",
    "        goal_index += 1\n",
    "        print(f\"Goal switched to: {goal_location}\")\n",
    "        prob = construct_problem(goal_location)  # rebuild with new goal\n",
    "        solver = cp.CLARABEL\n",
    "    \n",
    "\n",
    "    \n",
    "    initial_state.value = np.array(robot_state)\n",
    "    sqp_solutions = [previous_states]\n",
    "\n",
    "    \n",
    "    for i in range(num_sqp_iterations):\n",
    "        #print(\"goal location in sqp loop:\", goal_location)\n",
    "        As_value, Bs_value, Cs_value = jax.vmap(linearize, in_axes=[None, 0, 0, None])(dt_robot_dynamics, previous_states[:-1], previous_controls, 0.)\n",
    "        Gs_value = linearize_obstacle(previous_states, obstacle_location, obstacle_radius + robot_radius) \n",
    "        hs_value = jax.vmap(obstacle_constraint, [0, None, None])(previous_states, obstacle_location, obstacle_radius + robot_radius) - jax.vmap(jnp.dot, [0, 0])(Gs_value, previous_states)\n",
    "        #Gs_rect1_value = linearize_rect_smooth(previous_states, rectangles[0].center, rectangles[0].length, rectangles[0].width, rectangles[0].buffer) \n",
    "        #hs_rect1_value = jax.vmap(rect_constraint_smooth, [0, None, None, None, None])(previous_states, rectangles[0].center, rectangles[0].length, rectangles[0].width, rectangles[0].buffer) - jax.vmap(jnp.dot, [0, 0])(Gs_rect1_value, previous_states)       \n",
    "        \n",
    "        Gs_rects_values = []\n",
    "        hs_rects_values = []\n",
    "\n",
    "        for rect in rectangles:\n",
    "            G_val = linearize_rect_smooth(previous_states, rect.center, rect.length, rect.width, rect.buffer)\n",
    "            h_val = (\n",
    "                jax.vmap(rect_constraint_smooth, [0, None, None, None, None])(previous_states, rect.center, rect.length, rect.width, rect.buffer)\n",
    "                - jax.vmap(jnp.dot, [0, 0])(G_val, previous_states)\n",
    "            )\n",
    "            Gs_rects_values.append(G_val)\n",
    "            hs_rects_values.append(h_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        Goal_Gs_value = linearize_goal(previous_states, goal_location, goal_radius + robot_radius) \n",
    "        Goal_hs_value = jax.vmap(goal_constraint, [0, None, None])(previous_states, goal_location, goal_radius + robot_radius) - jax.vmap(jnp.dot, [0, 0])(Goal_Gs_value, previous_states)\n",
    "\n",
    "        for i in range(planning_horizon):\n",
    "            As[i].value = np.array(As_value[i])\n",
    "            Bs[i].value = np.array(Bs_value[i])\n",
    "            Cs[i].value = np.array(Cs_value[i])\n",
    "            Gs[i].value = np.array(Gs_value[i])\n",
    "            hs[i].value = np.array(hs_value[i:i+1])\n",
    "            #Gs_rect1[i].value = np.array(Gs_rect1_value[i])\n",
    "            #hs_rect1[i].value = np.array(hs_rect1_value[i:i+1])\n",
    "            Goal_Gs[i].value = np.array(Goal_Gs_value[i])\n",
    "            Goal_hs[i].value = np.array(Goal_hs_value[i:i+1])\n",
    "\n",
    "                # Handle all rectangle constraints\n",
    "            for r_idx in range(len(rectangles)):\n",
    "                Gs_rects[r_idx][i].value = np.array(Gs_rects_values[r_idx][i])\n",
    "                hs_rects[r_idx][i].value = np.array(hs_rects_values[r_idx][i:i+1])\n",
    "\n",
    "\n",
    "        Gs[planning_horizon].value = np.array(Gs_value[planning_horizon])\n",
    "        hs[planning_horizon].value = np.array(hs_value[planning_horizon:planning_horizon+1])\n",
    "        #Gs_rect1[planning_horizon].value = np.array(Gs_rect1_value[planning_horizon])\n",
    "        #hs_rect1[planning_horizon].value = np.array(hs_rect1_value[planning_horizon:planning_horizon+1])\n",
    "        Goal_Gs[planning_horizon].value = np.array(Goal_Gs_value[planning_horizon])\n",
    "        Goal_hs[planning_horizon].value = np.array(Goal_hs_value[planning_horizon:planning_horizon+1])\n",
    "\n",
    "        for r_idx in range(len(rectangles)):\n",
    "            Gs_rects[r_idx][planning_horizon].value = np.array(Gs_rects_values[r_idx][planning_horizon])\n",
    "            hs_rects[r_idx][planning_horizon].value = np.array(hs_rects_values[r_idx][planning_horizon:planning_horizon+1])\n",
    "\n",
    "\n",
    "\n",
    "        result = prob.solve(solver=solver)\n",
    "\n",
    "        if us.value is None:\n",
    "            print(\"No solution found\")\n",
    "            break\n",
    "        previous_controls = us.value\n",
    "        previous_states =  simulate_discrete_time_dynamics(dt_robot_dynamics, robot_state, previous_controls, 0., dt)\n",
    "        sqp_solutions.append(previous_states)\n",
    "        xs_previous.value = np.array(previous_states)\n",
    "        us_previous.value = np.array(previous_controls)\n",
    "    sqp_list.append(np.stack(sqp_solutions))\n",
    "    robot_control = previous_controls[0]\n",
    "    robot_control_list.append(robot_control)\n",
    "    \n",
    "    # get the robot next state using the control input\n",
    "    robot_state = dt_robot_dynamics(robot_state, robot_control, noises[t] * dt) \n",
    "    \n",
    "    # clipping the robot velocity so the problem doesn't become infeasible at the next step\n",
    "    robot_state = robot_state.at[3].set(jnp.clip(robot_state[3], v_min, v_max)) \n",
    "    \n",
    "    # add robot state and trajectory to the list\n",
    "    robot_trajectory.append(robot_state)\n",
    "    robot_trajectory_list.append(previous_states)\n",
    "    \n",
    "    # update the previous states and controls for the next iteration\n",
    "    previous_states =  simulate_discrete_time_dynamics(dt_robot_dynamics, robot_state, previous_controls, 0., dt)\n",
    "\n",
    "robot_trajectory = jnp.stack(robot_trajectory)\n",
    "robot_controls = jnp.stack(robot_control_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the results!\n",
    "\n",
    "(Just run the cell below)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "950bdae54c4040d8a6e00ecc1a0b0023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=74, description='i', max=149), IntSlider(value=7, description='j', max=1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_polygon_obstacle(polygon, ax):\n",
    "    polygon_loop = jnp.vstack([polygon, polygon[0]])\n",
    "    ax.plot(polygon_loop[:, 0], polygon_loop[:, 1], 'k-')\n",
    "    ax.fill(polygon_loop[:, 0], polygon_loop[:, 1], color='gray', alpha=0.5)\n",
    "\n",
    "# plotting the results. No need to add comments here. Just run this cell to visualize the results\n",
    "@interact(i=(0,num_time_steps-1), j=(0,num_sqp_iterations-1))\n",
    "\n",
    "def plot(i, j):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 4), gridspec_kw={'width_ratios': [2, 1]})\n",
    "    # fig, axs = plt.subplots(1,2, figsize=(10, 4))\n",
    "    ax = axs[0]\n",
    "    robot_position = robot_trajectory[i, :2]\n",
    "    circle1 = plt.Circle(robot_position, robot_radius, color='C0', alpha=0.4)\n",
    "    circle2 = plt.Circle(obstacle_location, obstacle_radius, color='C1', alpha=0.4)\n",
    "    #circle3 = plt.Circle(obstacle_location2, obstacle_radius, color='C1', alpha=0.4)\n",
    "    \n",
    "    # plot goals\n",
    "    for count in range(0, len(goal_locations)):\n",
    "        ax.add_patch(plt.Circle(goal_locations[count], goal_radius, color='C2', alpha=0.6))\n",
    "                \n",
    "        # Add goal index label at the center\n",
    "        ax.text(\n",
    "            goal_locations[count][0],                # x-coordinate\n",
    "            goal_locations[count][1],                # y-coordinate\n",
    "            str(count),                                 # text to display\n",
    "            color='black',                              # text color\n",
    "            fontsize=10,                                # size of the number\n",
    "            ha='center',                                # horizontal alignment\n",
    "            va='center',                                # vertical alignment\n",
    "            fontweight='bold'\n",
    "        )\n",
    "    \n",
    "\n",
    "\n",
    "    for count in range(0, len(rectangles)):\n",
    "        bottom_left = rectangles[count].center - jnp.array([rectangles[count].length / 2, rectangles[count].width / 2])\n",
    "        rect_plot = patches.Rectangle(\n",
    "            bottom_left,\n",
    "            rectangles[count].length,            # length along x\n",
    "            rectangles[count].width,            # width along y\n",
    "            linewidth=1,\n",
    "            edgecolor='C1',\n",
    "            facecolor='C1',\n",
    "            alpha=0.4\n",
    "        )\n",
    "        ax.add_patch(rect_plot)\n",
    "\n",
    "        # Add rectangle index label at the center\n",
    "        ax.text(\n",
    "            rectangles[count].center[0],                # x-coordinate\n",
    "            rectangles[count].center[1],                # y-coordinate\n",
    "            str(count),                                 # text to display\n",
    "            color='black',                              # text color\n",
    "            fontsize=10,                                # size of the number\n",
    "            ha='center',                                # horizontal alignment\n",
    "            va='center',                                # vertical alignment\n",
    "            fontweight='bold'\n",
    "        )\n",
    "\n",
    "    ax.add_patch(circle1)\n",
    "    ax.add_patch(circle2)\n",
    "    #ax.add_patch(circle3)\n",
    "    #plot_polygon_obstacle(polygon, ax)\n",
    "    ax.plot(robot_trajectory[:,0], robot_trajectory[:,1], \"o-\", markersize=3, color='grey')\n",
    "    ax.plot(robot_trajectory_list[i][:,0], robot_trajectory_list[i][:,1], \"o-\", markersize=3, color='red', label=\"Planned\")\n",
    "    # Plot planned trajectory for the selected SQP iteration\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #planned_trajectory = sqp_list[i][j]\n",
    "\n",
    "    if i < len(sqp_list) and j < len(sqp_list[i]):\n",
    "        planned_trajectory = sqp_list[i][j]\n",
    "        ax.plot(planned_trajectory[:, 0], planned_trajectory[:, 1], \"o-\", markersize=3, color='green', alpha=0.4, label=f\"SQP iteration {j}\")\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, f\"No SQP {j} at step {i}\", transform=ax.transAxes, ha='center', va='center', fontsize=12, color='red')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #ax.plot(planned_trajectory[:, 0], planned_trajectory[:, 1], \"o-\", markersize=3, color='green', alpha=0.4, label=\"SQP iteration %d\" % j)\n",
    "    ax.scatter(robot_trajectory[i:i+1,0], robot_trajectory[i:i+1,1], s=30,  color='C100', label=\"Robot\")\n",
    "    ax.set_xlim([-2, 7])\n",
    "    ax.grid()\n",
    "    ax.legend()\n",
    "    ax.axis(\"equal\")\n",
    "\n",
    "    ax.set_title(\"heading=%.2f velocity=%.2f\"%(robot_trajectory[i,2], robot_trajectory[i,3]))\n",
    "    \n",
    "    ax = axs[1]\n",
    "    ax.plot(robot_controls)\n",
    "    ax.scatter([i], robot_controls[i:i+1, 0], label=\"$tan(\\\\delta)$\", color='C0')\n",
    "    ax.scatter([i], robot_controls[i:i+1, 1], label=\"Acceleration\", color='C1')\n",
    "\n",
    "    ax.hlines(steering_min, 0, num_time_steps-1, color='C0', linestyle='--')\n",
    "    ax.hlines(steering_max, 0, num_time_steps-1, color='C0', linestyle='--')\n",
    "    ax.hlines(acceleration_min, 0, num_time_steps-1, color='C1', linestyle='--')\n",
    "    ax.hlines(acceleration_max, 0, num_time_steps-1, color='C1', linestyle='--')\n",
    "    \n",
    "    ax.plot(robot_trajectory[:,-1], markersize=3, color='C2')\n",
    "    ax.scatter([i], robot_trajectory[i:i+1, 3], label=\"Velocity\", color='C2')\n",
    "    ax.hlines(v_min, 0, num_time_steps-1, color='C2', linestyle='--')\n",
    "    ax.hlines(v_max, 0, num_time_steps-1, color='C2', linestyle='--')\n",
    "    ax.set_xlim([0, num_time_steps])\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.set_xlabel(\"Time step\")\n",
    "    ax.set_ylabel(\"Control\")\n",
    "    ax.set_title(\"Velocity, steering and acceleration\")\n",
    "    ax.legend()\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it guaranteed that your system will not hit the obstacle if we just apply MPC in the way we did above?\n",
    "If not, what are some techniques you could try to reduce the risk of colliding into the obstacles?\n",
    "\n",
    "(Optional) What if the obstacles were not stationary. But rather, they were moving. How would the problem change if the obstacle's motion were fully known (e.g., we knew exactly where the obstacle would be at any point in time), or if the obstacle's motion was uncertain (e.g., the obstacles were pedestrians, or space debris whose motion is not fully known). What are some techniques that could be applied to reduce the risk of collision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not guaranteed the system will not hit the obstacle. In my code, even with MPC applied, the first obstacle appears hit. In this case it was hit due to a sharp change in direction across only one timepoint. This indicates more safety measures should be put in place. Examples of those include increasing the size of the obstacle (similar to CBF applications) to compensate for error. While much more computationally intensive, the timestep frequency could be increased, which would make the magnitude of noise at each timestep smaller and give more opportunities for trajectory correction. Control restrictions can be put on the velocity to restrict movement, which also would give more opportunities for course correction. \n",
    "\n",
    "If the obstacles were moving but the motion was known, that should be able to be factored into the state and the problem should not change much. If the obstacle's motion was uncertain, then the planning horizon would need to be significantly decreased. A long planning horizon is useless if the states change unpredictably, and the desired trajectory may need to change on a whim. Adjusting the controls to allow for more leeway in sharp changes in acceleration would be helpful in dodging swift obstacles moving in the direction of the robot or its trajectory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ME548",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
